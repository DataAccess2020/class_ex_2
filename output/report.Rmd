---
title: "Class Exercise #2"
author: "Nicola Casarin & Francesco Catalfamo"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this short paper, we expose the workflow we built in order to complete the second class exercise of the Data Access and Regulation class.
Our goal was to implement the learned scraping techniques in a real-life scenario. The selected scraping target were the posts published during 2016 onto [Beppe Grillo's blog](https://beppegrillo.it/).

The team is composed of [Nicola Casarin](https://github.com/n-oise) and [Francesco Catalfamo](https://github.com/FCatalfamo).

## Tasks and Resolutions

#### Preparation

For our work we used six main packages:

```{r Preparation, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
library(httr)
library(curl)
library(here)
library(stringr)
```

Furthermore, in order to simplify the workflow we defined two functions: get_page() and page_name().

The first function scrape sites in a polite way and takes six arguments. *url*, a character vector with length 1 or more containing the links; *dest*, for defining the downloading path; *my_email*, for defining the scraper mail; *agent*, a logical if **TRUE** return the User Agent; *filetype*, for defining the output file type; lastly the *scrapeText*, a logical if **TRUE** scrape the download HTML content and assign it to an object.

```{r get_page}
get_page <- function(url, dest = "", my_email = "", agent = F, filetype = ".html", scrapeText = F) { 
  # Donwload file from a list of URLs controlling for http status code. 
    stopifnot(is.logical(agent))
    stopifnot(is.logical(scrapeText))
    UA <- ifelse(agent == FALSE, "", R.Version()$version.string)
    
    
    i <- 0
    for (i in 1:length(url)) {
      url_step <- url[i]
      httpReq <- httr::GET(url_step,
                     httr::add_headers(
                       From = my_email, 
                       `User-Agent` = UA
                       ))
      code <- httr::status_code(httpReq)
      
      
      # If status code is OK then:
      # download file with the filename and path as in page_name 
      if (code == 200) {
         bin <- httr::content(httpReq,
                        as = "raw")
         writeBin(object = bin,
                  con = page_name(url = url_step, dest, filetype))
         #Scrape the main text if specified 
         if (scrapeText == T | code == 200) {
           assign(basename(url_step), 
                  read_html(httpReq), 
                  envir = parent.frame())
         }
        
      } else if (code == 404) { # If status code is Not Found
        cat("Bad luck. Error ",
            code,
            " Resource not found! :(",
            sep = "")
       } else { # For every other status code report a generic error.
         cat("Error ",
             code,
             ". Check your URL!",
             sep = "")
       }
      
      # Sleep every round!
      Sys.sleep(2)
      
    }
}
```

The latter function is used inside the *get_page()* function and permits to defining the file name from the hostname and the basename. The function takes three arguments defined in the main *get_page()* function.

```{r page_name()}
page_name <- function(url, dest = "", filetype = ".html") {
  if (is.character(url)) {
    name <- stringr::str_extract(url, "[^https?://][^/]*")
    filename <- paste0("/",
                       name,
                       "_",
                       basename(url),
                       filetype)
    if (dest == "") { 
      #if the dest param is empty page_name define  path at the project's root
      if (name == basename(url)) {
      path <- paste0(here::here(),
                     "/",
                     basename(url),
                     filetype)
      } 
      else {
        path <- paste0(here::here(),
                       filename)
      } 
      return(path)
    } 
    else if (dest != ""){ 
      #if the dest param is NOT empty the function define 
      #the path to the project's subdirectory as in dest
      if (name == basename(url)) {
        path <- paste0(here::here(dest),
                       "/",
                       basename(url),
                       filetype)
      } 
      else {
        path <- paste0(here::here(dest),
                       filename)
      } 
      return(path)
    }
  } 
  else{
    cat("Not a string!")
   }
}
```

#### Task #1: Inspect the Robots.txt

```{r}
grillo <- 'https://beppegrillo.it/robots.txt'

get_page(grillo, 
         dest = "data", 
         my_email = "test@test.com",
         agent = T)
```         

To inspect the robots.txt we will use the *get_page()* function previously definied.
The function return 404 error which mean that beppegrillo.it blog does not have a robots.txt rules. For that reason, we will able to scrape all the site using a polite method. 

#### Task #2: Download the page in a polite way

```{r}
# Variables preparation ---------------------------------------------------
mare <- "https://beppegrillo.it/un-mare-di-plastica-ci-sommergera/"
email <-  "test@test.com"
UA <-  R.Version()$version.string

# Links scraping on the target page ---------------------------------------

#download site with specified headers
site <- RCurl::getURL(mare, 
                      httpheader = c(From = email, `User-Agent` = UA))
```    

Since there are many ways to download pages using R from the web, to download the targeted one we use *getURL()* from *RCurl* package. The page is stored in a R object named *site*. 

#### Task #3: Create a dataframe with all the links in the downloaded page

```{r}
#extraction of all link in the webpage
extract_links  <- read_html(site) %>% 
  html_elements(xpath = "//a/@href") %>% 
  html_text()

#creating empty df_links
df_links <- tibble(
  link = ""
)

#for loop appending the chr vector of links to the df
for (i in 1:length(extract_links)) {
  df_links <- rbind(df_links, extract_links[i])
}
df_links <- df_links %>% 
  transmute(link = str_extract(df_links$link, pattern = "^https?://beppegrillo.it/.*")) %>% 
  filter(!is.na(link)) %>%
  unique(.)
``` 

To extract all the links from the page, we use *rvest* package. The *html_elements()* permits us to extract specific elements from the file with the specified path.
To take the links which point to beppegrillo host site we make a for loop and then use *dplyr* package to clean the resulting data frame.
The regular expression used for extracting the links is ```^https?://beppegrillo.it/.*```.
The page contained 29 unique URLs.

#### Task #4: Download and scrape all the 2016 posts
```{r}
# Variables preparation ----------------------------------------------------
css <- ".td_module_10"
css2 <- ".entry-title.td-module-title a"

archive <- "https://beppegrillo.it/category/archivio/2016/page/"
my_email <- "test@test.com"
link_list <- vector()
```

To download all the 2016 posts we need to define the correct css selectors (*css*, *css2*) and the archive's page basic pattern.

```{r eval=FALSE}
# Link scraping loop  ----------------------------------------------------------
i <- 1
for (i in 1:47) {
  step <- paste0(archive, i)
  page <- httr::GET(url = step, 
            add_headers(
              From = my_email, 
              `User-Agent` = R.Version()$version.string
            ))
 links <-  content(page) %>%
    html_elements(css) %>%
    html_elements(css2) %>%
    html_attr(name = "href") 
 link_list <- append(x = link_list, values = links)
 Sys.sleep(2)
}
``` 

To get all the links we defined a for loop which went through all the 47 archive pages. After *GET* function, the loop extracted the posts links with two consecutive *html_elements* functions. Lastly, for all of the 47 pages we appended the links to *link_list* object.

```{undefined eval=FALSE, include=FALSE}
# Archive download --------------------------------------------------------
#We will download all 2016 post in a polite way and alongside scrape the text from the page 
get_page(url = link_list, 
         dest = "data",
         my_email = my_email,
         agent = T,
         scrapeText = T) 
``` 

We used the *get_page* function to download all the pages from the previous defined *link_list*. With the scrape text argument, we had scraped the main text into an object in the R environment. 
If we donwolad a page that does not contains any text we will download just the raw content. For instance, if a page contains just an imagine we will download its raw representation. 

#### Task #5: Sketch a spider scraper
```{r}

``` 

## Contributions

For this project we had made 26 commits, 3 active pull requests and 7 active issues.
Francesco has made 10 commits.
Nicola has made 16 commits.

